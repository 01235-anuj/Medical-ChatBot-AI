# -*- coding: utf-8 -*-
"""Medical ChatBot AI.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1x4urLM9JYckJQZVJ3cUcjLSmoDPGWinZ
"""

# Install dependencies

# Imports
import streamlit as st
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from transformers import pipeline
from langchain.llms import HuggingFacePipeline
from langchain.chains import RetrievalQA

# No !pip install here üö´



PDF_FILE = "Medical_Book.pdf"

# Load PDF
loader = PyPDFLoader(PDF_FILE)
documents = loader.load()

# Split text into chunks (smaller to avoid >512 token input)
text_splitter = CharacterTextSplitter(chunk_size=200, chunk_overlap=50)
docs = text_splitter.split_documents(documents)

# Embeddings
embeddings = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-MiniLM-L6-v2",
    model_kwargs={'device': -1}   # -1 means CPU for SentenceTransformer
)
# Vector DB
db = FAISS.from_documents(docs, embeddings)
retriever = db.as_retriever()

# Load Hugging Face model locally (no API calls)
generator = pipeline(
    "text2text-generation",
    model="google/flan-t5-large",
    max_length=512,
    device=-1   # -1 = CPU, 0 = GPU
)


# Wrap pipeline into LangChain
llm = HuggingFacePipeline(pipeline=generator)

qa = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=retriever,
    return_source_documents=False   # ‚úÖ only one output now
)

def ask_bot(query):
    try:
        result = qa.run(query)  # works fine now
        return result + "\n\n‚ö†Ô∏è Disclaimer: This is AI from Gale Encyclopedia, not medical advice."
    except Exception as e:
        return f"‚ö†Ô∏è Error: {str(e)}"



st.title("üöÄ Medical ChatBot AI")
user_input = st.text_input("Enter your medical question:")
if st.button("Get Answer"):
    answer = ask_bot(user_input)
    st.write(answer)

